ae<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Roberta Raileanu</title>
  
  <meta name="author" content="Roberta Raileanu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Roberta Raileanu</name>
              </p>
              <p>I am a PhD student in computer science at <a href="https://cs.nyu.edu/home/index.html">NYU</a>, advised by <a href="https://cs.nyu.edu/~fergus/pmwiki/pmwiki.php">Rob Fergus</a> as part of the <a href="https://wp.nyu.edu/cilvr/">CILVR</a> lab. 
                My research focuses on deep reinforcement learning. During my PhD, I was fortunate to spend time as a research intern at <a href="https://ai.facebook.com/">Facebook AI Research</a> and  <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a>. 
              </p>
              <p>
                Previously, I got my B.A. in Astrophysics from <a href="https://www.princeton.edu/">Princeton University</a>, where I worked with <a href="https://web.astro.princeton.edu/people/michael-strauss">Michael Strauss</a> on theoretical cosmology and <a href="https://www.astro.princeton.edu/~eco/">Eve Ostriker</a> on supernovae simulations.
                During high school, I had the chance to compete in international physics and astrophysics Olympiads , as part of Romania's national team. 
              </p>
              <p style="text-align:center">
                <a href="mailto:raileanu.roberta@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/RobertaRaileanu-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=9hVXpJ0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/rraileanu">GitHub</a> &nbsp/&nbsp
                <a href="https://twitter.com/robertarail">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/roberta-raileanu-44b25660/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/RobertaRaileanu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/RobertaRaileanu_headshot.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in designing machine learning algorithms that can make robust sequential decisions in complex environments. 
                My research spans various problems in reinforcement learning including exploration, fast adaptation to new environments, and multi-agent learning. 
                My current focus is on understanding and improving the generalization and robustness of reinforcement learning agents. 
              </p>
              <p>
                I'm always looking for collaborations, so if you're interested in working with me, please don't hesitate to get in touch! I'm also happy to advise masters or undergrad students on projects. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/ucbdrac.gif' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2006.12862">
                <papertitle>Automatic Data Augmentation for Generalization in Deep Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Max Goldstein, 
              Denis Yarats,
              Ilya Kostrikov, 
              Rob Fergus
              <br>
        <em>Inductive Biases, Invariances and Generalization in RL (BIG) Workshop</em>, <em>ICML</em>, 2020 <font color="darkorange"><strong>(oral)</strong></font>  
              <br>
              <a href="https://arxiv.org/pdf/2006.12862">paper</a>
        /
              <a href="https://github.com/rraileanu/auto-drac">code</a>
        /
              <a href="https://drive.google.com/file/d/1d86UogJuZuV5m3SqOoPEjKSH5OfOMimV/view?usp=sharing">slides</a>
        /
              <a href="https://sites.google.com/view/ucb-drac">project page</a>
              <p></p>
              <p>Use UCB to automatically select an augmentation from a given set, which is then used to regularize the policy and value function of an RL agent.</p>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/pdvf.gif' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2007.02879">
                <papertitle>Fast Adaptation via Policy-Dyamics Value Functions</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Max Goldstein, 
              Arthur Szlam, 
              Rob Fergus
              <br>
                <em>ICML</em>, 2020 
              <br>
                <em>Beyond "Tabula Rasa" in Reinforcement Learning (BeTR-RL) Workshop</em>, <em>ICLR</em>, 2020 <font color="darkorange"><strong>(oral)</strong></font>  
              <br>
              <a href="https://arxiv.org/abs/2007.02879">paper</a>
        /
              <a href="https://github.com/rraileanu/policy-dynamics-value-functions">code</a>
        /
              <a href="https://drive.google.com/file/d/1kIZKUnDYPu3HN1UPf8eAH6MLhGOmpEof/view">slides</a>
        /
              <a href="https://sites.google.com/view/policy-dynamics-value-function">project page</a>
              <p></p>
              <p>Learn a value function for a space of policies and environments (with different dynamics) and use it for fast adaptation in new environments with unseen dynamics.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/ride.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2002.12292">
                <papertitle>RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Tim Rocktäschel 
              <br>
                <em>ICLR</em>, 2020 
              <br>
              <a href="https://arxiv.org/abs/2002.12292">paper</a>
        /
              <a href="https://github.com/facebookresearch/impact-driven-exploration">code</a>
        /
              <a href="https://drive.google.com/file/d/1FR2NevHX6tcnSH2D-55IASPNIP62rhD5/view?usp=sharing">slides</a>
              <p></p>
              <p>Reward agents for taking actions that lead to large changes in the environment and for visiting new states within an episode.</p>
            </td>
          </tr> 
            
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/nethack.gif' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2006.13760">
                <papertitle>The NetHack Learning Environment</papertitle>
              </a>
              <br>
              Heinrich Küttler, 
              Nantas Nardelli, 
              Alexander H. Miller, 
              <strong>Roberta Raileanu</strong>,
              Marco Selvatici, 
              Edward Grefenstette, 
              Tim Rocktäschel
              <br>
                <em>NeurIPS</em>, 2020 
              <br>
                <em>Beyond "Tabula Rasa" in Reinforcement Learning (BeTR-RL) Workshop</em>, <em>ICLR</em>, 2020   
              <br>
              <a href="https://arxiv.org/abs/2006.13760">paper</a>
        /
              <a href="https://github.com/facebookresearch/nle">code</a>
        /
              <a href="https://drive.google.com/file/d/17-P9ZSVvcnHnRCLa7bJ61VGWTgrh2Lft/view?usp=sharing">slides</a>
              <p></p>
              <p>The NetHack Learning Environment (NLE) is a fast, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular game NetHack.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/amigo.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2006.12122">
                <papertitle>Learning with AMIGo: Adversarially Motivated Intrinsic Goals</papertitle>
              </a>
              <br>
              Andres Campero,  
              <strong>Roberta Raileanu</strong>,
              Heinrich Küttler, 
              Joshua B. Tenenbaum,
              Tim Rocktäschel,
              Edward Grefenstette, 
              <br>
                <em>preprint</em>, 2020   
              <br>
              <a href="https://arxiv.org/abs/2006.12122">paper</a>
        /
              <a href="https://github.com/facebookresearch/adversarially-motivated-intrinsic-goals">code</a>
              <p></p>
              <p>A teacher learns to generate goals at an appropriate level of difficulty for a student, creating an automatic curriculum that aids exploration.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/backplay.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1807.06919">
                <papertitle>Backplay: "Man muss immer umkehren"</papertitle>
              </a>
              <br>
              Cinjon Resnick*, 
              <strong>Roberta Raileanu*</strong>,
              Sanyam Kapoor, 
              Alexander Peysakhovich, 
              Kyunghyun Cho, 
              Joan Bruna
              <br>
                <em>Reinforcement Learning in Games Workshop</em>, <em>AAAI</em>, 2019   
              <br>
              <a href="https://arxiv.org/abs/1807.06919">paper</a>
        /
              <a href="https://drive.google.com/file/d/1-4GS0CYkoEXUOKh2myDHF0CD57KcBRmF/view?usp=sharing">slides</a>
              <p></p>
              <p>Create a curriculum by initializing the RL agent along a single demonstration (either optimal or suboptimal) starting near the end of the trajectory.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/som.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf">
                <papertitle>Modeling Others using Oneself in Multi-Agent Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Emily Denton,
              Arthur Szlam,
              Rob Fergus
              <br>
                <em>ICML</em>, 2018  
              <br>
                <em>Emergent Communication Workshop</em>, <em>NeurIPS</em>, 2017 
              <br>
              <a href="http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf">paper</a>
        /
              <a href="https://drive.google.com/file/d/1LryOkLqUD3Hp462Zx5c0w0_MHxD-kFzv/view?usp=sharing">slides</a>
              <p></p>
              <p>Simulate other agents' behavior and infer their intentions by using your own policy.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/supernovae.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1610.03092">
                <papertitle>Superbubbles in the Multiphase ISM and the Loading of Galactic Winds</papertitle>
              </a>
              <br>
              Chang-Goo Kim, 
              Eve C. Ostriker,
              <strong>Roberta Raileanu</strong>,
              <br>
                <em>The Astrophysical Journal</em>, 2016  
              <br>
              <a href="https://arxiv.org/abs/1610.03092">paper</a>
              <p></p>
              <p>Use numerical simulations to analyze the evolution and properties of superbubbles, driven by supernovae, that propagate into the two-phase, cloudy interstellar medium.</p>
            </td>
          </tr> 

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                
                <a href="https://jonbarron.info/">website credits</a>
                
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
