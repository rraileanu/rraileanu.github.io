<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Roberta Raileanu</title>
  
  <meta name="author" content="Roberta Raileanu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Roberta Raileanu</name>
              </p>
              <p>I am a Research Scientist at <a href="https://ai.facebook.com/">FAIR (Meta)</a> based in London, working on building generally capable autonomous agents. In 2021, I obtained my PhD in Computer Science from <a href="https://cs.nyu.edu/home/index.html">NYU</a>, advised by <a href="https://cs.nyu.edu/~fergus/pmwiki/pmwiki.php">Rob Fergus</a>. My focus was on deep reinforcement learning.
                During my PhD, I was fortunate to intern at <a href="https://deepmind.com/">DeepMind</a>, <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a>, and <a href="https://ai.facebook.com/">Facebook AI Research</a>. 
              </p>
              <p>
                Previously, I obtained a B.A. in Astrophysics from <a href="https://www.princeton.edu/">Princeton University</a>, where I worked with <a href="https://web.astro.princeton.edu/people/michael-strauss">Michael Strauss</a> on theoretical cosmology and <a href="https://www.astro.princeton.edu/~eco/">Eve Ostriker</a> on supernovae simulations.
              </p>
              <p style="text-align:center">
                <a href="mailto:raileanu.roberta@gmail.com">Email</a> &nbsp/&nbsp
                <a href="CV_Resume_Feb_2023.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=462OoekAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/rraileanu">GitHub</a> &nbsp/&nbsp
                <a href="https://twitter.com/robertarail">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/roberta-raileanu-44b25660/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/headshot_green.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/headshot_green.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am broadly interested in designing machine learning algorithms that can make robust sequential decisions in complex environments, while constantly acquiring new skills and knowledge. My long-term goal is to develop models that can solve a wide range of new tasks with little or no supervision and training. I also care deeply about ensuring such systems are reliable, trustworthy, and aligned with human intentions.
              </p>                
              <p>
                My work draws from fields such as reinforcement learning, open-ended learning, self-supervised learning, and natural language processing. Currently, I work at the intersection of large language models (LLMs) and decision making / RL. In particular, I focus on augmenting LLMs with actions, tools, and goals, and teaching them to learn from (human) feedback and interaction. 
              </p>
              <p>
              I'm also interested in understanding and improving the generalization and robustness of autonomous agents and large pretrained models. In the past, I've worked on exploration, fast adaptation, learning from demonstrations, as well as continual and multi-agent learning.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/toolformer.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2302.04761.pdf">
                <papertitle>Toolformer: Language Models Can Teach Themselves to Use Tools</papertitle>
              </a>
              <br>
              Timo Schick,  Jane Dwivedi-Yu, Roberto Dessi, <strong>Roberta Raileanu</strong>, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom
              <br>
                <em>arXiv</em>, 2023 
              <br>
              <a href="https://arxiv.org/pdf/2302.04761.pdf">paper</a>
              <p></p>
              <p>A method that teaches language models to use tools in a self-supervised way.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/alms.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2302.07842.pdf">
                <papertitle>Augmented Language Models: a Survey</papertitle>
              </a>
              <br>
              Grégoire Mialon et al.
              <br>
                <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2302.07842.pdf">paper</a>
              <p></p>
              <p>Survey on augmenting language models with reasoning, actions, and tool use.</p>
            </td>
            
            
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/csp.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2211.10445">
                <papertitle>Building a Subspace of Policies for Scalable Continual Learning</papertitle>
              </a>
              <br>
              Jean-Baptiste Gaya,
              Thang Doan, 
              Lucas Caccia, 
              Laure Soulier, 
              Ludovic Denoyer,        
              <strong>Roberta Raileanu</strong>
              <br>
                <em>ICLR</em>, 2023 <font color="darkorange"><strong>(spotlight, top-25%)</strong></font>  
              <br>
              <a href="https://arxiv.org/abs/2211.10445">paper</a>
              <p></p>
              <p>Introduce a continual reinforcement learning method that incrementally builds a subspace of policies and adaptively prunes it to preserve a good trade-off between model size and performance.</p>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/nld.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2211.00539">
                <papertitle>Dungeons and Data: A Large-Scale NetHack Dataset</papertitle>
              </a>
              <br>
              Eric Hambro, 
              <strong>Roberta Raileanu</strong>,
              Danielle Rothermel,
              Vegard Mella,
              Tim Rocktäschel              
              Heinrich Küttler, 
              Naila Murray
              <br>
                <em>NeurIPS</em>, 2022 
              <br>
              <a href="https://arxiv.org/abs/2211.00539">paper</a>
        /
              <a href="https://github.com/facebookresearch/nle">code</a>
              <p></p>
              <p>Present the NetHack Learning Dataset (NLD), a large and highly scalable dataset of human and bot trajectories on the popular game of NetHack.</p>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/e3b.gif' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.05805">
                <papertitle>Exploration via Elliptical Episodic Bonuses</papertitle>
              </a>
              <br>
              Mikael Henaff, <strong>Roberta Raileanu</strong>, Minqi Jiang, Tim Rocktäschel
              <br>
                <em>NeurIPS</em>, 2022 
              <br>
              <a href="https://arxiv.org/abs/2210.05805">paper</a>
        /
              <a href="https://github.com/facebookresearch/e3b">code</a>
        /
              <a href="https://e3bagent.github.io/">website</a>
              <p></p>
              <p>Extend episodic count-based bonuses to continuous state spaces for better exploration of contextual MDPs with high-dimensional observations.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/lamigo.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.10330">
                <papertitle>Improving Intrinsic Exploration with Language Abstractions</papertitle>
              </a>
              <br>
              Jesse Mu, Victor Zhong, <strong>Roberta Raileanu</strong>, Minqi Jiang, Noah Goodman, Tim Rocktaschel, Edward Grefenstette
              <br>
                <em>NeurIPS</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2202.08938">paper</a>
              <p></p>
              <p>Using language to highlight relevant state abstractions leads to better exploration in sparse reward procedurally generated environments.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/xland.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.10330">
                <papertitle>Open-Ended Learning Leads to Generally Capable Agents</papertitle>
              </a>
              <br>
              Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, <strong>Roberta Raileanu</strong>, Steph Hughes-Fitt, Valentin Dalibard, Wojciech Marian Czarnecki
              <br>
              <a href="https://arxiv.org/abs/2107.12808">paper</a>
              <p></p>
              <p>Training agents on dynamically changing task distributions leads to more general agents capable of solving a wide range of tasks in procedurally generated environments.</p>
            </td>
          </tr> 
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/idaac.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.10330">
                <papertitle>Decoupling Value and Policy for Generalization in Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Rob Fergus
              <br>
                <em>ICML</em>, 2021 <font color="darkorange"><strong>(oral)</strong></font>  
              <br>
              <a href="https://arxiv.org/abs/2102.10330">paper</a>
        /
              <a href="https://github.com/rraileanu/idaac">code</a>
              <p></p>
              <p>Using a common representation for the policy and value function can lead to overfitting in deep reinforcement learning. To improve generalization, use the advantage instead of the value as auxiliary loss to train the policy network, while encouraging the representation to be invariant to task-irrelevant properties of the environment.</p>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/ucbdrac.gif' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2006.12862">
                <papertitle>Automatic Data Augmentation for Generalization in Deep Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Max Goldstein, 
              Denis Yarats,
              Ilya Kostrikov, 
              Rob Fergus
              <br>
                <em>NeurIPS</em>, 2021 
              <br>
                <em>Inductive Biases, Invariances and Generalization in RL (BIG) Workshop</em>, <em>ICML</em>, 2020 <font color="darkorange"><strong>(oral)</strong></font>  
              <br>
              <a href="https://arxiv.org/pdf/2006.12862">paper</a>
        /
              <a href="https://github.com/rraileanu/auto-drac">code</a>
        /
              <a href="https://drive.google.com/file/d/1d86UogJuZuV5m3SqOoPEjKSH5OfOMimV/view?usp=sharing">slides</a>
        /
              <a href="https://sites.google.com/view/ucb-drac">website</a>
              <p></p>
              <p>Use UCB to automatically select an augmentation from a given set, which is then used to regularize the policy and value function of an RL agent.</p>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/pdvf.gif' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2007.02879">
                <papertitle>Fast Adaptation via Policy-Dyamics Value Functions</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Max Goldstein, 
              Arthur Szlam, 
              Rob Fergus
              <br>
                <em>ICML</em>, 2020 
              <br>
                <em>Beyond "Tabula Rasa" in Reinforcement Learning (BeTR-RL) Workshop</em>, <em>ICLR</em>, 2020 <font color="darkorange"><strong>(oral)</strong></font>  
              <br>
              <a href="https://arxiv.org/abs/2007.02879">paper</a>
        /
              <a href="https://github.com/rraileanu/policy-dynamics-value-functions">code</a>
        /
              <a href="https://drive.google.com/file/d/1kIZKUnDYPu3HN1UPf8eAH6MLhGOmpEof/view">slides</a>
        /
              <a href="https://sites.google.com/view/policy-dynamics-value-function">website</a>
              <p></p>
              <p>Learn a value function for a space of policies and environments (with different dynamics) and use it for fast adaptation in new environments with unseen dynamics.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/ride.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2002.12292">
                <papertitle>RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Tim Rocktäschel 
              <br>
                <em>ICLR</em>, 2020 
              <br>
              <a href="https://arxiv.org/abs/2002.12292">paper</a>
        /
              <a href="https://github.com/facebookresearch/impact-driven-exploration">code</a>
        /
              <a href="https://drive.google.com/file/d/1FR2NevHX6tcnSH2D-55IASPNIP62rhD5/view?usp=sharing">slides</a>
              <p></p>
              <p>Reward agents for taking actions that lead to large changes in the environment and for visiting new states within an episode.</p>
            </td>
          </tr> 
            
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/nethack.gif' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2006.13760">
                <papertitle>The NetHack Learning Environment</papertitle>
              </a>
              <br>
              Heinrich Küttler, 
              Nantas Nardelli, 
              Alexander H. Miller, 
              <strong>Roberta Raileanu</strong>,
              Marco Selvatici, 
              Edward Grefenstette, 
              Tim Rocktäschel
              <br>
                <em>NeurIPS</em>, 2020 
              <br>
                <em>Beyond "Tabula Rasa" in Reinforcement Learning (BeTR-RL) Workshop</em>, <em>ICLR</em>, 2020   
              <br>
              <a href="https://arxiv.org/abs/2006.13760">paper</a>
        /
              <a href="https://github.com/facebookresearch/nle">code</a>
        /
              <a href="https://drive.google.com/file/d/17-P9ZSVvcnHnRCLa7bJ61VGWTgrh2Lft/view?usp=sharing">slides</a>
              <p></p>
              <p>The NetHack Learning Environment (NLE) is a fast, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular game NetHack.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/amigo.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2006.12122">
                <papertitle>Learning with AMIGo: Adversarially Motivated Intrinsic Goals</papertitle>
              </a>
              <br>
              Andres Campero,  
              <strong>Roberta Raileanu</strong>,
              Heinrich Küttler, 
              Joshua B. Tenenbaum,
              Tim Rocktäschel,
              Edward Grefenstette, 
              <br>
                <em>ICLR</em>, 2021   
              <br>
              <a href="https://arxiv.org/abs/2006.12122">paper</a>
        /
              <a href="https://github.com/facebookresearch/adversarially-motivated-intrinsic-goals">code</a>
              <p></p>
              <p>A teacher learns to generate goals at an appropriate level of difficulty for a student, creating an automatic curriculum that aids exploration.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/backplay.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1807.06919">
                <papertitle>Backplay: "Man muss immer umkehren"</papertitle>
              </a>
              <br>
              Cinjon Resnick*, 
              <strong>Roberta Raileanu*</strong>,
              Sanyam Kapoor, 
              Alexander Peysakhovich, 
              Kyunghyun Cho, 
              Joan Bruna
              <br>
                <em>Reinforcement Learning in Games Workshop</em>, <em>AAAI</em>, 2019   
              <br>
              <a href="https://arxiv.org/abs/1807.06919">paper</a>
        /
              <a href="https://drive.google.com/file/d/1-4GS0CYkoEXUOKh2myDHF0CD57KcBRmF/view?usp=sharing">slides</a>
              <p></p>
              <p>Create a curriculum by initializing the RL agent along a single demonstration (either optimal or suboptimal) starting near the end of the trajectory.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/som.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf">
                <papertitle>Modeling Others using Oneself in Multi-Agent Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Emily Denton,
              Arthur Szlam,
              Rob Fergus
              <br>
                <em>ICML</em>, 2018  
              <br>
                <em>Emergent Communication Workshop</em>, <em>NeurIPS</em>, 2017 
              <br>
              <a href="http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf">paper</a>
        /
              <a href="https://drive.google.com/file/d/1LryOkLqUD3Hp462Zx5c0w0_MHxD-kFzv/view?usp=sharing">slides</a>
              <p></p>
              <p>Simulate other agents' behavior and infer their intentions by using your own policy.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/supernovae.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1610.03092">
                <papertitle>Superbubbles in the Multiphase ISM and the Loading of Galactic Winds</papertitle>
              </a>
              <br>
              Chang-Goo Kim, 
              Eve C. Ostriker,
              <strong>Roberta Raileanu</strong>,
              <br>
                <em>The Astrophysical Journal</em>, 2016  
              <br>
              <a href="https://arxiv.org/abs/1610.03092">paper</a>
              <p></p>
              <p>Use numerical simulations to analyze the evolution and properties of superbubbles, driven by supernovae, that propagate into the two-phase, cloudy interstellar medium.</p>
            </td>
          </tr> 

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                
                <a href="https://jonbarron.info/">website credits</a>
                
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
