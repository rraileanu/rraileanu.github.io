<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Roberta Raileanu</title>
  
  <meta name="author" content="Roberta Raileanu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Roberta Raileanu</name>
              </p>
              <p>I am a Research Scientist at <a href="https://ai.facebook.com/">Meta AI Research</a> (FAIR) based in London, working on deep reinforcement learning. In 2021, I obtained my PhD in Computer Science from <a href="https://cs.nyu.edu/home/index.html">NYU</a>, advised by <a href="https://cs.nyu.edu/~fergus/pmwiki/pmwiki.php">Rob Fergus</a>. 
                During my PhD, I was fortunate to spend time as a research intern at <a href="https://deepmind.com/">DeepMind</a>, <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a>, and <a href="https://ai.facebook.com/">Facebook AI Research</a>. 
              </p>
              <p>
                Previously, I obtained a B.A. in Astrophysics from <a href="https://www.princeton.edu/">Princeton University</a>, where I worked with <a href="https://web.astro.princeton.edu/people/michael-strauss">Michael Strauss</a> on theoretical cosmology and <a href="https://www.astro.princeton.edu/~eco/">Eve Ostriker</a> on supernovae simulations.
              </p>
              <p style="text-align:center">
                <a href="mailto:raileanu.roberta@gmail.com">Email</a> &nbsp/&nbsp
                <a href="Roberta_CV_Apr_2022.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=9hVXpJ0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/rraileanu">GitHub</a> &nbsp/&nbsp
                <a href="https://twitter.com/robertarail">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/roberta-raileanu-44b25660/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/headshot_green.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/headshot_green.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am broadly interested in designing machine learning algorithms that can make robust sequential decisions in complex environments, while constantly acquiring new skills and knowledge. 
                In the past, I've worked on various problems in reinforcement learning including learning from demonstrations, exploration of procedurally generated environments, fast adaptation to new dynamics, and multi-agent learning. 
              </p>
              <p>
                My current focus is on understanding and improving the generalization and robustness of reinforcement learning agents. 
                In particular, I am interested in understanding how we can best leverage self-supervised and open-ended learning techniques to continually expand the range of capabilities for RL agents in diverse and non-stationary environments. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/lamigo.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.10330">
                <papertitle>Improving Intrinsic Exploration with Language Abstractions</papertitle>
              </a>
              <br>
              Jesse Mu, Victor Zhong, <strong>Roberta Raileanu</strong>, Minqi Jiang, Noah Goodman, Tim Rocktaschel, Edward Grefenstette
              <br>
                <em>under review</em>
              <br>
              <a href="https://arxiv.org/abs/2202.08938">paper</a>
              <p></p>
              <p>Using language to highlight relevant state abstractions leads to better exploration in sparse reward procedurally generated environments.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/xland.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.10330">
                <papertitle>Open-Ended Learning Leads to Generally Capable Agents</papertitle>
              </a>
              <br>
              Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, <strong>Roberta Raileanu</strong>, Steph Hughes-Fitt, Valentin Dalibard, Wojciech Marian Czarnecki
              <br>
              <a href="https://arxiv.org/abs/2107.12808">paper</a>
              <p></p>
              <p>Training agents on dynamically changing task distributions leads to more general agents capable of solving a wide range of tasks in procedurally generated environments.</p>
            </td>
          </tr> 
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/idaac.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.10330">
                <papertitle>Decoupling Value and Policy for Generalization in Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Rob Fergus
              <br>
                <em>ICML</em>, 2021 <font color="darkorange"><strong>(oral)</strong></font>  
              <br>
              <a href="https://arxiv.org/abs/2102.10330">paper</a>
              <p></p>
              <p>Using a common representation for the policy and value function can lead to overfitting in deep reinforcement learning. To improve generalization, use the advantage instead of the value as auxiliary loss to train the policy network, while encouraging the representation to be invariant to task-irrelevant properties of the environment.</p>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/ucbdrac.gif' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2006.12862">
                <papertitle>Automatic Data Augmentation for Generalization in Deep Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Max Goldstein, 
              Denis Yarats,
              Ilya Kostrikov, 
              Rob Fergus
              <br>
                <em>NeurIPS</em>, 2021 
              <br>
                <em>Inductive Biases, Invariances and Generalization in RL (BIG) Workshop</em>, <em>ICML</em>, 2020 <font color="darkorange"><strong>(oral)</strong></font>  
              <br>
              <a href="https://arxiv.org/pdf/2006.12862">paper</a>
        /
              <a href="https://github.com/rraileanu/auto-drac">code</a>
        /
              <a href="https://drive.google.com/file/d/1d86UogJuZuV5m3SqOoPEjKSH5OfOMimV/view?usp=sharing">slides</a>
        /
              <a href="https://sites.google.com/view/ucb-drac">project page</a>
              <p></p>
              <p>Use UCB to automatically select an augmentation from a given set, which is then used to regularize the policy and value function of an RL agent.</p>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/pdvf.gif' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2007.02879">
                <papertitle>Fast Adaptation via Policy-Dyamics Value Functions</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Max Goldstein, 
              Arthur Szlam, 
              Rob Fergus
              <br>
                <em>ICML</em>, 2020 
              <br>
                <em>Beyond "Tabula Rasa" in Reinforcement Learning (BeTR-RL) Workshop</em>, <em>ICLR</em>, 2020 <font color="darkorange"><strong>(oral)</strong></font>  
              <br>
              <a href="https://arxiv.org/abs/2007.02879">paper</a>
        /
              <a href="https://github.com/rraileanu/policy-dynamics-value-functions">code</a>
        /
              <a href="https://drive.google.com/file/d/1kIZKUnDYPu3HN1UPf8eAH6MLhGOmpEof/view">slides</a>
        /
              <a href="https://sites.google.com/view/policy-dynamics-value-function">project page</a>
              <p></p>
              <p>Learn a value function for a space of policies and environments (with different dynamics) and use it for fast adaptation in new environments with unseen dynamics.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/ride.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2002.12292">
                <papertitle>RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Tim Rocktäschel 
              <br>
                <em>ICLR</em>, 2020 
              <br>
              <a href="https://arxiv.org/abs/2002.12292">paper</a>
        /
              <a href="https://github.com/facebookresearch/impact-driven-exploration">code</a>
        /
              <a href="https://drive.google.com/file/d/1FR2NevHX6tcnSH2D-55IASPNIP62rhD5/view?usp=sharing">slides</a>
              <p></p>
              <p>Reward agents for taking actions that lead to large changes in the environment and for visiting new states within an episode.</p>
            </td>
          </tr> 
            
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/nethack.gif' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2006.13760">
                <papertitle>The NetHack Learning Environment</papertitle>
              </a>
              <br>
              Heinrich Küttler, 
              Nantas Nardelli, 
              Alexander H. Miller, 
              <strong>Roberta Raileanu</strong>,
              Marco Selvatici, 
              Edward Grefenstette, 
              Tim Rocktäschel
              <br>
                <em>NeurIPS</em>, 2020 
              <br>
                <em>Beyond "Tabula Rasa" in Reinforcement Learning (BeTR-RL) Workshop</em>, <em>ICLR</em>, 2020   
              <br>
              <a href="https://arxiv.org/abs/2006.13760">paper</a>
        /
              <a href="https://github.com/facebookresearch/nle">code</a>
        /
              <a href="https://drive.google.com/file/d/17-P9ZSVvcnHnRCLa7bJ61VGWTgrh2Lft/view?usp=sharing">slides</a>
              <p></p>
              <p>The NetHack Learning Environment (NLE) is a fast, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular game NetHack.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/amigo.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2006.12122">
                <papertitle>Learning with AMIGo: Adversarially Motivated Intrinsic Goals</papertitle>
              </a>
              <br>
              Andres Campero,  
              <strong>Roberta Raileanu</strong>,
              Heinrich Küttler, 
              Joshua B. Tenenbaum,
              Tim Rocktäschel,
              Edward Grefenstette, 
              <br>
                <em>ICLR</em>, 2021   
              <br>
              <a href="https://arxiv.org/abs/2006.12122">paper</a>
        /
              <a href="https://github.com/facebookresearch/adversarially-motivated-intrinsic-goals">code</a>
              <p></p>
              <p>A teacher learns to generate goals at an appropriate level of difficulty for a student, creating an automatic curriculum that aids exploration.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/backplay.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1807.06919">
                <papertitle>Backplay: "Man muss immer umkehren"</papertitle>
              </a>
              <br>
              Cinjon Resnick*, 
              <strong>Roberta Raileanu*</strong>,
              Sanyam Kapoor, 
              Alexander Peysakhovich, 
              Kyunghyun Cho, 
              Joan Bruna
              <br>
                <em>Reinforcement Learning in Games Workshop</em>, <em>AAAI</em>, 2019   
              <br>
              <a href="https://arxiv.org/abs/1807.06919">paper</a>
        /
              <a href="https://drive.google.com/file/d/1-4GS0CYkoEXUOKh2myDHF0CD57KcBRmF/view?usp=sharing">slides</a>
              <p></p>
              <p>Create a curriculum by initializing the RL agent along a single demonstration (either optimal or suboptimal) starting near the end of the trajectory.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/som.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf">
                <papertitle>Modeling Others using Oneself in Multi-Agent Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Roberta Raileanu</strong>,
              Emily Denton,
              Arthur Szlam,
              Rob Fergus
              <br>
                <em>ICML</em>, 2018  
              <br>
                <em>Emergent Communication Workshop</em>, <em>NeurIPS</em>, 2017 
              <br>
              <a href="http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf">paper</a>
        /
              <a href="https://drive.google.com/file/d/1LryOkLqUD3Hp462Zx5c0w0_MHxD-kFzv/view?usp=sharing">slides</a>
              <p></p>
              <p>Simulate other agents' behavior and infer their intentions by using your own policy.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/supernovae.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1610.03092">
                <papertitle>Superbubbles in the Multiphase ISM and the Loading of Galactic Winds</papertitle>
              </a>
              <br>
              Chang-Goo Kim, 
              Eve C. Ostriker,
              <strong>Roberta Raileanu</strong>,
              <br>
                <em>The Astrophysical Journal</em>, 2016  
              <br>
              <a href="https://arxiv.org/abs/1610.03092">paper</a>
              <p></p>
              <p>Use numerical simulations to analyze the evolution and properties of superbubbles, driven by supernovae, that propagate into the two-phase, cloudy interstellar medium.</p>
            </td>
          </tr> 

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                
                <a href="https://jonbarron.info/">website credits</a>
                
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
